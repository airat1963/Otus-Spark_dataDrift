{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5614bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f138d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6bd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84f0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df):\n",
    "    \"\"\"Функция для валидации загруженных данных\"\"\"\n",
    "    print(\"\\n=== Проверка качества данных ===\")\n",
    "    \n",
    "    # 1. Проверка на пропущенные значения\n",
    "    print(\"\\nПропущенные значения:\")\n",
    "    df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "    \n",
    "    # 2. Проверка уникальности transaction_id\n",
    "    print(\"\\nДубликаты transaction_id:\")\n",
    "    df.groupBy(\"transaction_id\").count() \\\n",
    "      .filter(\"count > 1\") \\\n",
    "      .count() \\\n",
    "      .show()\n",
    "    \n",
    "    # 3. Проверка допустимых диапазонов значений\n",
    "    print(\"\\nНекорректные суммы транзакций:\")\n",
    "    df.filter(col(\"tx_amount\") <= 0).count()\n",
    "    \n",
    "    # 4. Проверка временного диапазона\n",
    "    print(\"\\nВременной диапазон транзакций:\")\n",
    "    df.select(min(\"tx_datetime\"), max(\"tx_datetime\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71de1765",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5a13047f570f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Загрузка данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[1;32m    136\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1569\u001b[0m             answer, self._gateway_client, None, self._fqn)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        # Загрузка данных\n",
    "        df = spark.read.parquet(\"s3a://fraud-detection-data-otus-2025/parquet/*\")\n",
    "        \n",
    "        # Проверка данных\n",
    "        validate_data(df)\n",
    "        \n",
    "        # Сохранение проверенных данных\n",
    "        df.write.mode(\"overwrite\") \\\n",
    "           .parquet(\"s3a://fraud-detection-data-otus-2025/validated_data/\")\n",
    "        \n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    " spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7a2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, min, max\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f4cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(iteration, total, prefix='', suffix='', length=50, fill='█'):\n",
    "    \"\"\"\n",
    "    Выводит прогресс-бар в консоль\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.1f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar = fill * filled_length + '-' * (length - filled_length)\n",
    "    sys.stdout.write(f'\\r{prefix} |{bar}| {percent}% {suffix}')\n",
    "    sys.stdout.flush()\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d744c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df):\n",
    "    \"\"\"Функция для валидации загруженных данных\"\"\"\n",
    "    print(\"\\n=== Проверка качества данных ===\")\n",
    "    \n",
    "    # 1. Проверка на пропущенные значения\n",
    "    print(\"\\n[1/4] Проверка пропущенных значений...\")\n",
    "    start_time = time.time()\n",
    "    missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    missing_values.show(truncate=False)\n",
    "    print(f\"Проверка завершена за {time.time() - start_time:.2f} сек\")\n",
    "    \n",
    "    # 2. Проверка уникальности transaction_id\n",
    "    print(\"\\n[2/4] Проверка дубликатов transaction_id...\")\n",
    "    start_time = time.time()\n",
    "    duplicates_count = df.groupBy(\"transaction_id\").count().filter(\"count > 1\").count()\n",
    "    print_progress(1, 1, prefix='Прогресс:', suffix='Завершено', length=50)\n",
    "    print(f\"Найдено дубликатов: {duplicates_count}\")\n",
    "    print(f\"Проверка завершена за {time.time() - start_time:.2f} сек\")\n",
    "    \n",
    "    # 3. Проверка допустимых диапазонов значений\n",
    "    print(\"\\n[3/4] Проверка некорректных сумм транзакций...\")\n",
    "    start_time = time.time()\n",
    "    invalid_amounts = df.filter(col(\"tx_amount\") <= 0).count()\n",
    "    print_progress(1, 1, prefix='Прогресс:', suffix='Завершено', length=50)\n",
    "    print(f\"Найдено транзакций с неположительной суммой: {invalid_amounts}\")\n",
    "    print(f\"Проверка завершена за {time.time() - start_time:.2f} сек\")\n",
    "    \n",
    "    # 4. Проверка временного диапазона\n",
    "    print(\"\\n[4/4] Проверка временного диапазона транзакций...\")\n",
    "    start_time = time.time()\n",
    "    time_range = df.select(min(\"tx_datetime\"), max(\"tx_datetime\"))\n",
    "    print_progress(1, 1, prefix='Прогресс:', suffix='Завершено', length=50)\n",
    "    time_range.show(truncate=False)\n",
    "    print(f\"Проверка завершена за {time.time() - start_time:.2f} сек\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38970c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64eb7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инициализация Spark сессии...\n",
      "Spark сессия успешно создана\n",
      "\n",
      "=== Загрузка данных ===\n",
      "[1/2] Чтение данных из S3...\n",
      "Прогресс: |██████████████████████████████████████████████████| 100.0% Завершено\n",
      "Загружено 657923861 строк\n",
      "Время загрузки: 16.43 сек\n",
      "\n",
      "\n",
      "=== Проверка качества данных ===\n",
      "\n",
      "[1/4] Проверка пропущенных значений...\n",
      "+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|transaction_id|tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|14            |1365       |14         |17262      |14       |14             |14          |14      |14               |\n",
      "+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "\n",
      "Проверка завершена за 76.47 сек\n",
      "\n",
      "[2/4] Проверка дубликатов transaction_id...\n",
      "Прогресс: |██████████████████████████████████████████████████| 100.0% Завершено\n",
      "Найдено дубликатов: 729\n",
      "Проверка завершена за 181.54 сек\n",
      "\n",
      "[3/4] Проверка некорректных сумм транзакций...\n",
      "Прогресс: |██████████████████████████████████████████████████| 100.0% Завершено\n",
      "Найдено транзакций с неположительной суммой: 12247\n",
      "Проверка завершена за 20.05 сек\n",
      "\n",
      "[4/4] Проверка временного диапазона транзакций...\n",
      "Прогресс: |██████████████████████████████████████████████████| 100.0% Завершено\n",
      "+-------------------+-------------------+\n",
      "|min(tx_datetime)   |max(tx_datetime)   |\n",
      "+-------------------+-------------------+\n",
      "|2019-11-20 00:00:00|2021-01-12 23:59:59|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "Проверка завершена за 33.36 сек\n",
      "\n",
      "=== Сохранение данных ===\n",
      "[2/2] Запись проверенных данных в S3...\n",
      "Прогресс: |██████████████████████████████████████████████████| 100.0% Завершено\n",
      "Данные успешно сохранены\n",
      "Время сохранения: 857.84 сек\n",
      "\n",
      "Завершение Spark сессии...\n",
      "Spark сессия остановлена\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Инициализация Spark сессии...\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    print(\"Spark сессия успешно создана\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Загрузка данных\n",
    "        print(\"=== Загрузка данных ===\")\n",
    "        print(\"[1/2] Чтение данных из S3...\")\n",
    "        start_time = time.time()\n",
    "        df = spark.read.parquet(\"s3a://fraud-detection-data-otus-2025/parquet/*\")\n",
    "        print_progress(1, 1, prefix='Прогресс:', suffix='Завершено', length=50)\n",
    "        print(f\"Загружено {df.count()} строк\")\n",
    "        print(f\"Время загрузки: {time.time() - start_time:.2f} сек\\n\")\n",
    "        \n",
    "        # Проверка данных\n",
    "        validate_data(df)\n",
    "        \n",
    "        # Сохранение проверенных данных\n",
    "        print(\"\\n=== Сохранение данных ===\")\n",
    "        print(\"[2/2] Запись проверенных данных в S3...\")\n",
    "        start_time = time.time()\n",
    "        df.write.mode(\"overwrite\").parquet(\"s3a://fraud-detection-data-otus-2025/validated_data/\")\n",
    "        print_progress(1, 1, prefix='Прогресс:', suffix='Завершено', length=50)\n",
    "        print(f\"Данные успешно сохранены\")\n",
    "        print(f\"Время сохранения: {time.time() - start_time:.2f} сек\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! ОШИБКА: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"\\nЗавершение Spark сессии...\")\n",
    "        spark.stop()\n",
    "        print(\"Spark сессия остановлена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f0dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# время записи увеличено, т.к не хватило место, и спарк похоже оптимизировал процесс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2aa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c89c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe446d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3692353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779f820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a623d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Инициализация Spark\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Batch_TXT_to_Parquet_Conversion\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\") \\\n",
    "#     .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "#     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация Spark с оптимизацией для больших файлов    .config(\"spark.sql.parquet.compression.codec\", \"zstd\") \\ \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Massive_TXT_to_Parquet_Conversion\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"access_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"secret_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"zstd\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"2g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01817ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Схема данных\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType()),\n",
    "    StructField(\"tx_datetime\", TimestampType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"terminal_id\", IntegerType()),\n",
    "    StructField(\"tx_amount\", DoubleType()),\n",
    "    StructField(\"tx_time_seconds\", IntegerType()),\n",
    "    StructField(\"tx_time_days\", IntegerType()),\n",
    "    StructField(\"tx_fraud\", IntegerType()),\n",
    "    StructField(\"tx_fraud_scenario\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3db429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути\n",
    "input_bucket = \"s3a://otus-mlops-data17/\"\n",
    "output_bucket = \"s3a://fraud-detection-data-otus-2025/parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Список файлов для конвертации\n",
    "all_files = [\n",
    "    \"2019-08-22.txt\",\n",
    "    \"2019-09-21.txt\",\n",
    "    \"2019-10-21.txt\",\n",
    "    \"2019-11-20.txt\",\n",
    "    \"2019-12-20.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a123ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Список файлов для конвертации\n",
    "all_files = [ \n",
    "    \"2020-02-18.txt\",\n",
    "    \"2020-04-18.txt\",\n",
    "    \"2020-07-17.txt\",\n",
    "    \"2020-08-16.txt\",\n",
    "    \"2020-10-15.txt\",\n",
    "    \"2020-11-14.txt\",\n",
    "    \"2020-12-14.txt\"\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbb1124",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Wrong FS: s3a://otus-mlops-data17/, expected: hdfs://rc1a-dataproc-m-aq3b14utvqh59ufq.mdb.yandexcloud.net",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-86852db0a3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Получаем список всех файлов динамически\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m all_files = [f.name for f in spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoopConfiguration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Wrong FS: s3a://otus-mlops-data17/, expected: hdfs://rc1a-dataproc-m-aq3b14utvqh59ufq.mdb.yandexcloud.net"
     ]
    }
   ],
   "source": [
    "# Получаем список всех файлов динамически\n",
    "all_files = [f.name for f in spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "    spark.sparkContext._jsc.hadoopConfiguration()\n",
    ").listStatus(\n",
    "    spark.sparkContext._jvm.org.apache.hadoop.fs.Path(input_bucket)\n",
    ") if f.getPath().getName().endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc77584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Сортируем по дате в имени файла\n",
    "all_files_sorted = sorted(all_files, key=lambda x: datetime.strptime(x.split('.')[0], \"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f6546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для конвертации с прогресс-баром\n",
    "def convert_with_progress(files):\n",
    "    from tqdm import tqdm\n",
    "    success_count = 0\n",
    "    \n",
    "    for file in tqdm(files, desc=\"Конвертация файлов\"):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            df = spark.read \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"delimiter\", \",\") \\\n",
    "                .schema(transaction_schema) \\\n",
    "                .csv(f\"{input_bucket}{file}\")\n",
    "            \n",
    "            # Оптимизированная запись с партиционированием\n",
    "            df.repartition(8).write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"compression\", \"zstd\") \\\n",
    "                .parquet(f\"{output_bucket}{file.replace('.txt', '')}\")\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            tqdm.write(f\"{file} -> {file.replace('.txt', '')} | {df.count():,} строк | {elapsed:.1f} сек\")\n",
    "            success_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Ошибка в {file}: {str(e)}\")\n",
    "    \n",
    "    return success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab58c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало конвертации 7 файлов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Конвертация файлов:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Запуск конвертации\n",
    "print(f\"Начало конвертации {len(all_files_sorted)} файлов...\")\n",
    "success = convert_with_progress(all_files_sorted[:40])  # Ограничение 40 файлов\n",
    "print(f\"Успешно сконвертировано {success} из {len(all_files_sorted[:40])} файлов\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
